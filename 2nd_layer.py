# -*- coding: utf-8 -*-
"""Directors cut. "2-й слой нейронной сети. Методология научных исследований.ipynb"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JwEJjZR24WmaYXcevZsCzYVtZantQl18
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

#from my_optimizer import optimizer_step

from matplotlib import pyplot as plt
from matplotlib import pyplot, cm
from mpl_toolkits.mplot3d import Axes3D

def f(x):
    return 0.

def analytic_solution(x):
    sol =  (1 / (np.exp(np.pi) - np.exp(-np.pi))) * \
           np.sin(np.pi * x[0]) * (np.exp(np.pi * x[1]) - np.exp(-np.pi * x[1]))
    return sol

from torch.autograd.functional import jacobian
from torch.autograd.functional import hessian
from torch.autograd import grad

def loss_function(x, y,pde,psy_trial,f):
    loss_sum = 0.

    for xi in x:
        for yi in y:
            input_point = torch.Tensor([xi, yi]).double()
            input_point.requires_grad_()

            net_out = pde.forward(input_point)
            net_out_w = grad(outputs=net_out, inputs=pde.fc1.weight, grad_outputs=torch.ones_like(net_out),
                       retain_graph=True,create_graph=True)


            net_out_jacobian = jacobian(pde.forward,input_point,create_graph=True)
            # jac1  = get_jacobian(pde.forward,input_point,2)
            net_out_hessian = hessian(pde.forward,input_point,create_graph=True)
            psy_t = psy_trial(input_point, net_out)

            inputs = (input_point, net_out)
            psy_t_jacobian = jacobian(psy_trial, inputs,create_graph=True)[0]
            psy_t_hessian  = hessian(psy_trial,inputs,create_graph=True)
            psy_t_hessian = psy_t_hessian[0][0]
            # acobian(jacobian(psy_trial))(input_point, net_out

            gradient_of_trial_d2x = psy_t_hessian[0][0]
            gradient_of_trial_d2y = psy_t_hessian[1][1]

            # D_gradient_of_trial_d2x_D_W0 = grad(outputs=gradient_of_trial_d2x, inputs=pde.fc1.weight, grad_outputs=torch.ones_like(gradient_of_trial_d2x), retain_graph=True)
            # D_gradient_of_trial_d2y_D_W0 = grad(outputs=gradient_of_trial_d2y, inputs=pde.fc1.weight, grad_outputs=torch.ones_like(gradient_of_trial_d2y), retain_graph=True)
            # D_func_D_W0 = grad(outputs=func,iputs=pde.fc1.weight,grad_outputs=torch.ones_like(func))
            func = f(input_point)
            func_t = torch.Tensor([func])
            func_t.requires_grad_()

            err_sqr = ((gradient_of_trial_d2x + gradient_of_trial_d2y) - func_t) ** 2
            # D_err_sqr_D_W0 = 2*((gradient_of_trial_d2x + gradient_of_trial_d2y) - func)*(
            #                     (D_gradient_of_trial_d2x_D_W0 + D_gradient_of_trial_d2y_D_W0) -D_func_D_W0
            #                     )

            loss_sum += err_sqr
            qq = 0

    return loss_sum

import numpy as np
# одинаковые у всех значения весов для упрощения проверки
w0 = np.array([[-0.45183179, -0.9692577,  -0.69703202], [-0.94012508, -0.38095557,  2.15272962]])
w1 = np.array([ 0.70225393,  0.54302488, -3.01373014])

import torch.nn as nn
# нейронная сеть для решения уравнения Пуассона
class PDEnet(nn.Module):
    def __init__(self,N):
        super(PDEnet,self).__init__()
        self.N = N
        fc1 = nn.Linear(2,self.N) # первый слой
        fc1.weight = nn.Parameter(torch.from_numpy(w0).T.double())
        fc1.bias = nn.Parameter(torch.zeros(fc1.bias.shape).double())
        print(fc1.bias)
        fc2 = nn.Linear(self.N, 1) # второй слой
        fc2.weight = nn.Parameter(torch.from_numpy(w1).reshape(1,w1.shape[0]).double())
        fc2.bias = nn.Parameter(torch.zeros(fc2.bias.shape).double())
        self.fc1 = fc1

        self.fc2 = fc2

    def forward(self,x):
        x = x.reshape(1, 2)
        y = self.fc1(x)
        y = torch.sigmoid(y)
        y = self.fc2(y.reshape(1, self.N))
        return y

def A(x):
    return (x[1] * torch.sin(np.pi * x[0]))

def psy_trial(x, net_out):
    return A(x) + x[0] * (1 - x[0]) * x[1] * (1 - x[1]) * net_out

nx = 3
ny = nx
pde = PDEnet(nx)
dx = 1. / nx
dy = 1. / ny

x_space = torch.linspace(0, 1, nx).double()
y_space = torch.linspace(0, 1, ny).double()
print("CUDA GPU:", torch.cuda.is_available())
if torch.cuda.is_available():
  print(torch.device)
  print(x_space.device)
  x_space.to("cuda:0")
  #y_space.to("cuda:0")
  #print(x_space)
  print(x_space.device)

pde.fc1.bias

pde.fc2.bias

input_point = torch.zeros(2).double()

input_point.shape,pde.fc1.weight.T.shape

pde.fc1.weight.dtype,input_point.dtype,pde.fc1.bias.dtype

torch.matmul(input_point,pde.fc1.weight.T)

out1 = pde.fc1(input_point)
out1

torch.matmul(out1.reshape(1,3),pde.fc2.weight.reshape(1,3).T)

pde.fc2(out1)

net_out = pde.forward(input_point)
net_out

psy_t = psy_trial(input_point, net_out)
print(psy_t)

#loss = loss_function(x_space, y_space,pde,psy_trial,f)
#loss.backward()
print(x_space.grad)

# lmb = 0.001
# optimizer = torch.optim.SGD(pde.parameters(), lr=lmb)
# import time
# t1 = time.time()
# for i in range(100):
#     #print('begin ',i,loss.item())
#     optimizer.zero_grad()
#     #print('zero grad ',i,loss.item())
#     #print(x_space.device,y_space.device)
#     loss = loss_function(x_space, y_space,pde,psy_trial,f)
#     #print(loss.device)
#     print(i,loss.item())
#     loss.backward(retain_graph=True)
#     #print('loop end ',i,loss.item())
#     optimizer.step()
#
#     print('step ',i,loss.item())
# t2 = time.time()
# print('computation time ',t2-t1)

surface = np.zeros((nx,ny))
an_surface = np.zeros((nx,ny))
for i, x in enumerate(x_space):
    for j, y in enumerate(y_space):
        input_point = torch.Tensor([x, y]).double()
        input_point.requires_grad_()
        net_out = pde.forward(input_point)

        psy_t = psy_trial(input_point, net_out)
        surface[i][j] = psy_t
        an_surface[i][j] = analytic_solution([x, y])
diff = np.max(np.abs(surface-an_surface))
print(diff)

import matplotlib.pyplot as plt
import numpy as np
fig = plt.figure()
fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
X, Y = np.meshgrid(x_space.numpy(), y_space.numpy())
surf = ax.plot_surface(X, Y, surface, rstride=1, cstride=1, cmap=cm.viridis,
                       linewidth=0, antialiased=False)
plt.title('Neural solution')

fig = plt.figure()
fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
X, Y = np.meshgrid(x_space.numpy(), y_space.numpy())
surf = ax.plot_surface(X, Y, an_surface, rstride=1, cstride=1, cmap=cm.viridis,
                       linewidth=0, antialiased=False)
plt.title('Analytic solution')

from sklearn.metrics import mean_absolute_percentage_error
mape = mean_absolute_percentage_error(an_surface, surface)
mape

import numpy as np
diff = np.abs(an_surface-surface)
md = np.max(diff)
md

# ШАГ 1.Вычислите значение 1-го слоя нейронной сети (на Numpy, пока ничего не меняя),
# в точке с коодинатами np.ones(2)

x = torch.ones(2).double()
y1 = pde.fc1(x)
print(y1)

#ШАГ 2. преобразуйте тензор x из размерности 2 в размерность (2,1)
xt = x.reshape(2,1)

# ШАГ 3. Добавьте тензорам w (w = pde.fc1.weight) и xt две новых фиктивных размерности
# для перехода к пакетной обработке.
# используйте для этого функцию reshape по следующему образцу
# x = x.reshape(1,1,x.shape[0],x.shape[1])
w = pde.fc1.weight
W = w.reshape(1,1,3,2)
Xt = xt.reshape(1,1,2,1)
print(W.shape,X.shape)

# ШАГ 4. Продублируйте с помощью функции repeat полученные на шаге 3 тензора по первым двух измерениям
# nx, ny раз
W = W.repeat(3,3,1,1)
Xt = Xt.repeat(3,3,1,1)

# ШАГ 5. С помощью функции np.meshgrid преобразуте массивы точек x_space,y_space
# в двумерный формат. Созданные массивы X,Y преобразуйте в тензора
X, Y = np.meshgrid(x_space.numpy(), y_space.numpy())
X = torch.from_numpy(X)
Y = torch.from_numpy(Y)

# ШАГ 6. Для дальнейших вычислений добавьте тензорам X,Y третье измерение с размерностью 1
X = X.reshape(X.shape[0],X.shape[1],1)
Y = Y.reshape(Y.shape[0],Y.shape[1],1)

# ШАГ 7. По третьему измерению с помощью функции torch.cat соедините тензора X,Y.
# Результат назовите XY
XY = torch.cat((X,Y),dim=2)
XY

# ШАГ 8. Добавьте тензору XY фиктивное 4-е измерение с размерностью 1
XY.shape
XY = XY.reshape(XY.shape[0],Y.shape[1],XY.shape[2],1)

XY.shape

torch.matmul(XY[0][0].T,pde.fc1.weight.T)

pde.fc1.weight,pde.fc1.bias

pde.fc1(XY[0][0].T)

XY[0][0].T,W[0][0].T

W[0][0].T,pde.fc1.weight.T

pde.fc1(XY[0][0].T)

W.shape, XY.shape

# ШАГ 9. Выполните пакетное умножение матриц (или умножение блочных матриц)
# четвертого ранга w и XY с помощью функции torch.einsum.
# Результат представляет собой вычисление первого слоя нейронной сети во двумерной области.
res = torch.einsum('abij,abjk->abi', W, XY)
print(res.shape)
res

# ШАГ 9. Сравните значения, полученные на предыдущем шаге с результатами выполнения
# прямого хода нейронной сети (функция pde.forward) для каждой точки двумерной области (тензор XY_2D = XY.reshape(XY.shape[0],XY.shape[1],XY.shape[2])

XY_2D = XY.reshape(XY.shape[0],XY.shape[1],XY.shape[2])

x = XY_2D[0][0]
x.shape

res[0][0]

# проверка правильности вычисления 1-го слоя нейронной сети во всей области
  # с помощью расчета по точкам
t_manual = torch.zeros_like(res)
for i,x in enumerate(x_space):
      for j,y in enumerate(y_space):
          t = torch.tensor([x,y]).double()
          y1 = pde.fc1(t)
          t_manual[j,i,:] = y1        # зачем нужно переставлять индексы?

          print(t,y1)

t_manual

torch.max(torch.abs(t_manual-res)).item()

# функция активации
res_activated = torch.sigmoid(res)

"""2-й слой: fc2 = nn.Linear(self.N, 1)"""

# ШАГ 2-1. Аналогично предыдущему, создайте матрицу весов 2-го слоя,
#          продублировав вес в отдельной точке w1 для всех точек области

w1 = torch.from_numpy(w1)

w11 =w1.reshape(1,1,w1.shape[0])
w11.shape

W1 = w11.repeat(3,3,1)

w1

W1

# ШАГ 2-2. С помощью функции einsum выполните пакетное умножение весов 2-го слоя
          #  на активированные выходные значения 1-го слоя

W1.shape,res_activated.shape

# W1 = W1.reshape(W1.shape[0],W1.shape[1],W1.shape[2],1)
# res_activated = res_activated.reshape(W1.shape[0],W1.shape[1],W1.shape[2],1)

W1.shape,res_activated.shape

res2 = torch.einsum('abi,abi->ab', W1, res_activated)

res2

pde.forward(XY[0][0])

# проверка правильности вычисления 1-го слоя нейронной сети во всей области
  # с помощью расчета по точкам
t_manual2 = torch.zeros_like(res2)
for i,x in enumerate(x_space):
      for j,y in enumerate(y_space):
          t = torch.tensor([x,y]).double()
          y1 = pde(t)
          t_manual2[j,i] = y1        # нужно ли переставлять индексы?

t_manual2

torch.max(torch.abs(res2-t_manual2))

# Функция потерь
# ШАГ 3-1. вычисление значений нейросети во всей области
#  для одной точки: net_out = pde.forward(input_point)

net_out_all  = torch.einsum('abi,abi->ab', W1,
                                  torch.sigmoid(
                                         torch.einsum('abij,abjk->abi', W, XY)
                                              )
)

def forward_all(w,w1,x):
    res = torch.einsum('abi,abi->ab', w1,
                   torch.sigmoid(
                       torch.einsum('abij,abjk->abi', w, x)
                   )
                   )
    return res



net_out_all

pde(XY[0][0])

net_out_all = forward_all(W,W1,XY)

torch.max(torch.abs(net_out_all-t_manual2))
qq = 0